# LocalLLM動作確認チェックリスト

## ✅ 完了した項目

### 1. モデルインストール確認
- ✅ `qwen2.5:7b` - インストール済み（4.7 GB）
- ✅ `qwen2.5:14b` - インストール済み（9.0 GB）
- ✅ Ollama動作確認 - 正常に動作（テストで「8」を正しく返答）

### 2. 環境変数設定
- ✅ `.env.local` ファイル作成済み
- ✅ 設定内容確認：
  ```
  AI_PROVIDER=local
  LOCAL_LLM_API_URL=http://localhost:11434/api/generate
  LOCAL_LLM_MODEL_CLASSIFY=qwen2.5:7b
  LOCAL_LLM_MODEL_REPORT=qwen2.5:14b
  ```

### 3. 開発サーバー起動
- ✅ `npm run dev` 実行中（バックグラウンド）

---

## 🔍 動作確認項目

### ブラウザでの確認（http://localhost:3000）

#### 1. 基本動作確認
- [ ] ページが正常に表示される
- [ ] ログイン画面が表示される
- [ ] ログインが正常に動作する

#### 2. 分類機能の確認
- [ ] 実習記録を入力して送信
- [ ] 分類結果が1-12のカテゴリで返ってくる
- [ ] レスポンス時間が3-5秒程度
- [ ] 分類理由が日本語で適切に表示される

#### 3. 長文対応の確認
- [ ] 200文字以上の長文を入力
- [ ] 要素分解が正常に動作する
- [ ] 複数要素が個別に分類される

#### 4. レポート生成の確認
- [ ] 10件以上の投稿がある状態でレポート生成
- [ ] レポートが300-400文字で生成される
- [ ] レポート内容が適切な日本語で生成される

#### 5. エラー確認
- [ ] ブラウザのコンソールにエラーがない
- [ ] ネットワークタブでAPI呼び出しが成功している
- [ ] サーバーログにエラーがない

---

## 🐛 トラブルシューティング

### 問題1: サーバーが起動しない
**確認事項**:
- ポート3000が使用されていないか確認
- `npm install` が完了しているか確認

### 問題2: 分類が動作しない
**確認事項**:
- Ollamaが起動しているか確認（`ollama list`）
- `.env.local` の設定が正しいか確認
- ブラウザのコンソールでエラーメッセージを確認

### 問題3: レスポンスが遅い
**確認事項**:
- GPUが正常に動作しているか確認（タスクマネージャー）
- モデルが正しくロードされているか確認

### 問題4: 分類結果が不正確
**確認事項**:
- プロンプトが正しく送信されているか確認
- モデルの応答を直接確認（`ollama run qwen2.5:7b`）

---

## 📊 期待される動作

### 分類API (`/api/classify-advanced`)
- **入力**: 実習記録テキスト
- **出力**: 
  ```json
  {
    "isMultiple": false,
    "category": 8,
    "reason": "LocalLLM分類: カテゴリ8",
    "confidence": 0.8
  }
  ```
- **レスポンス時間**: 3-5秒

### レポート生成API (`/api/generate-report`)
- **入力**: 10件以上の投稿データ
- **出力**: 300-400文字の振り返りレポート
- **レスポンス時間**: 10-15秒

---

## 🎯 次のステップ

1. ブラウザで `http://localhost:3000` にアクセス
2. 上記のチェックリストを順番に確認
3. 問題があれば、トラブルシューティングを参照
4. すべて正常に動作すれば、実装完了！

---

**作成日**: 2025年12月2日
**ステータス**: 環境設定完了、動作確認待ち

