# LocalLLM モデル選択ガイド

## PCスペック
- **GPU**: NVIDIA GeForce RTX 4070 Ti SUPER 16GB
- **RAM**: 32GB
- **CPU**: Intel Core i9-14900F
- **ストレージ**: 954GB

## プロジェクト要件

### 1. 分類タスク（基本・最重要）
- **タスク**: 12時計分類（1-12のカテゴリ判定）
- **入力**: 医学実習体験のテキスト
- **出力**: カテゴリ番号（1-12）のみ
- **特徴**: シンプルで明確な分類タスク、プロンプトが構造化されている

### 2. 長文要素分解
- **タスク**: 200文字以上の長文を複数要素に分解
- **特徴**: 各要素を個別に分類する必要がある

### 3. レポート生成
- **タスク**: 300-400文字の振り返りレポート生成
- **特徴**: 複数の投稿を分析して要約、最も複雑なタスク

---

## 推奨モデル選択

### 🎯 **推奨構成（バランス重視）**

#### **分類タスク用**: Qwen2.5 7B
- **理由**:
  - 日本語対応が非常に良好
  - 7Bモデルで16GB VRAMに余裕がある
  - 分類タスクには十分な性能
  - 高速な応答速度
- **Ollamaモデル名**: `qwen2.5:7b`
- **VRAM使用量**: 約5-6GB
- **推奨用途**: `/api/classify.js`, `/api/classify-advanced.js` の分類部分

#### **レポート生成用**: Qwen2.5 14B
- **理由**:
  - 日本語対応が良好
  - 14Bモデルで16GB VRAMに収まる
  - レポート生成に十分な性能
  - より複雑なタスクに対応可能
- **Ollamaモデル名**: `qwen2.5:14b`
- **VRAM使用量**: 約10-12GB
- **推奨用途**: `/api/generate-report.js`, `/api/classify-advanced.js` の要素分解部分

### 🔄 **代替案1（軽量重視）**

すべてのタスクに **Qwen2.5 7B** を使用
- **メリット**: 高速、VRAM使用量が少ない
- **デメリット**: レポート生成の品質がやや低下する可能性

### 🔄 **代替案2（高品質重視）**

すべてのタスクに **Qwen2.5 14B** を使用
- **メリット**: すべてのタスクで高品質
- **デメリット**: 分類タスクがやや遅くなる可能性

### 🔄 **代替案3（最高品質）**

- **分類**: Qwen2.5 7B
- **レポート生成**: Llama 3.1 70B（量子化版）
- **注意**: 70Bモデルは量子化が必要（Q4_K_M推奨）

---

## モデル比較表

| モデル | サイズ | VRAM使用量 | 日本語対応 | 分類性能 | レポート性能 | 速度 |
|--------|--------|-----------|-----------|---------|------------|------|
| **Qwen2.5 7B** | 7B | ~5-6GB | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **Qwen2.5 14B** | 14B | ~10-12GB | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| **Llama 3.1 8B** | 8B | ~5-6GB | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **Mistral 7B** | 7B | ~5-6GB | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **Mixtral 8x7B** | 47B (MoE) | ~14-16GB | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |

---

## 最終推奨

### ✅ **推奨構成（実用的）**

**分類タスク**: `qwen2.5:7b`
**レポート生成**: `qwen2.5:14b`

この構成により：
- 分類タスクは高速で正確
- レポート生成は高品質
- VRAM使用量を最適化
- 日本語対応が良好

### 📥 モデルのインストール

```bash
# 分類タスク用モデル
ollama pull qwen2.5:7b

# レポート生成用モデル
ollama pull qwen2.5:14b
```

### 🔧 実装時の設定

環境変数でモデルを切り替え可能にする：

```bash
# .env.local
AI_PROVIDER=local
LOCAL_LLM_API_URL=http://localhost:11434/api/generate
LOCAL_LLM_MODEL_CLASSIFY=qwen2.5:7b
LOCAL_LLM_MODEL_REPORT=qwen2.5:14b
```

---

## 注意事項

1. **初回起動時**: モデルのダウンロードに時間がかかります（7B: ~4.5GB, 14B: ~8.5GB）
2. **VRAM使用量**: 2つのモデルを同時に使用する場合は、合計で約15-18GBのVRAMを使用します
3. **メモリ不足時**: 分類タスクのみにQwen2.5 7Bを使用することを検討してください
4. **パフォーマンステスト**: 実装後、実際のタスクで性能を確認し、必要に応じて調整してください

---

## 次のステップ

1. モデルのインストール（上記コマンド実行）
2. モデルの動作確認（Ollamaで直接テスト）
3. 実装開始（`lib/ai-providers.js` の `classifyWithLocal()` を実装）


